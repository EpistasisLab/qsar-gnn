%\documentclass[wsdraft]{ws-procs11x85}

\documentclass{ws-procs11x85}
\usepackage{ws-procs-thm}           % comment this line when `amsthm / theorem / ntheorem` package is used

\begin{document}

\title{Machine Learning on Publicly Available Semantic Graph Data for Interpretable QSAR Modeling}

\author{Joseph~D.~Romano$^*$, Yun~Hao$^*$, and Jason~H.~Moore$^\dag$}

\address{Institute for Biomedical Informatics, University of Pennsylvania,\\
Philadelphia, Pennsylvania 19104, United States\\
$^\dag$E-mail: jhmoore@upenn.edu\\
$^*$These authors contributed equally.}

\begin{abstract}
This is a placeholder for the real abstract.
Please write each sentence on its own line in the \LaTeX{} source file for easier tracking in Git.
\end{abstract}

\keywords{Toxicology; Graph machine learning; QSAR; Artificial Intelligence.}

\section{Introduction}\label{aba:sec1}
Testing.

\section{Methods}

\subsection{Data sources}
The data used in this study come from a new data resource for computational toxicology, named ComptoxAI.
ComptoxAI includes a large graph database containing many entity and relationship types that pertain to translational mechanisms of toxicity, all of which are sourced from third-party public databases (including PubChem, Drugbank, the US EPA's Computational Toxicology Dashboard, NCBI Gene, and many others).
We extracted the subgraph from ComptoxAI's graph database defined as all nodes representing chemicals, genes, and toxicological assays, as well as the complete set of edges linking nodes of those types.
A metagraph describing the node and edge types in the subgraph is shown in [FIGURE XXX].

\subsection{Heterogeneous graph neural network}
We constructed a heterogeneous graph neural network (GNN) architecture for the graph ML experiments.
Since our approach uses multiple entity types (e.g., chemicals, genes, assays) in the same graph---each with possibly different sets of node features, and linked by multiple semantically distinct edge types---the architecture extends the common GCN model to learn separate message passing functions for each edge type.
We use a message passing strategy similar to GraphSAGE~\cite{hamilton2017inductive}, where signals propagated from adjacent nodes are aggregated using their arithmetic mean:
\begin{equation}
\mathbf{h}_v^k \gets \sigma\left(\mathbf{W} \cdot \textsc{MEAN}\left(\{\mathbf{h}_v^{k-1}\}\cup \{ \mathbf{h}_u^{k-1},\forall u \in \mathcal{N}(v) \}\right)\right)
\end{equation}
where $\mathbf{h}_v^{k}$ is an encoded representation of the input vector $\mathbf{x}$ of node $v$ at layer $k$ of the network, $\mathbf{W}$ is a weight matrix, and $\mathcal{N}(v)$ is the neighborhood of all nodes adjacent to $v$.
Each layer in the network `pulls' information from an increasingly wider radius around each node $v$.
$\sigma$ is an activation applied to the output of each layer; we use the leaky ReLU function:
\begin{equation}
\sigma(x) =
\begin{cases}
   x & \quad \text{if } x > 0\\
   0.01x & \quad \text{otherwise}
\end{cases}
\end{equation}

\subsubsection{Node prediction model}

\subsubsection{Edge prediction model}

\subsection{Baseline QSAR models}

\section{Results}

\section{Discussion}

\section{Conclusions}

\section{Code availability}
All source code pertaining to this study is available on GitHub at \url{https://github.com/JDRomano2/psb-gnn}, and in a `frozen' version on FigShare at [XXX].

\bibliographystyle{ws-procs11x85}
\bibliography{psb-gnn}

\end{document} 

%%% \renewcommand\bibname{References\\ {\normalfont\it References can be typed in your preferred bibliography style.}}